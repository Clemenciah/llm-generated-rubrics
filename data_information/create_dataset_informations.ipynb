{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnTlxS8RAI6V"
      },
      "source": [
        "# In this notebook, we will explicitly create the essential informations for each of the datasets we aim to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fB3ByJsAI6X"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxctjM8DAI6X"
      },
      "source": [
        "For the purpose of this project, we will be using the following datasets:\n",
        "1. USR\n",
        "2. SumPubMed\n",
        "3. SummEval\n",
        "4. HelpSteer2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDlLkU5GAI6X"
      },
      "source": [
        "### Definement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVllU0atAI6Y"
      },
      "source": [
        "For the metric definement phase we will be needing the following materials:\n",
        "1. Task description (used when asking the LLM to generate metrics)\n",
        "2. Dataset description\n",
        "3. Main data (when using samples in generation)\n",
        "4. An indicator of the column that will act as the main quality label\n",
        "5. An indicator of the content column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNjnt1KiAI6Z"
      },
      "source": [
        "### Scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xyj_BtFAI6Z"
      },
      "source": [
        "For this phase, in addition to the previously mentioned data, we will also use the following data:\n",
        "1. Test sample\n",
        "2. Ground Truth metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIMUTXmLAI6Z"
      },
      "source": [
        "## Datasets and Their Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s7QhiERAI6Z"
      },
      "source": [
        "### USR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjdT6qHNAI6b"
      },
      "source": [
        "#### Definement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tINcSjeTAI6b"
      },
      "outputs": [],
      "source": [
        "content_column_name = \"context\"\n",
        "response_column_name = \"response\"\n",
        "overall_score_column_name = \"Overall\"\n",
        "default_task_description = \"You will be given a conversation between two individuals. \" \\\n",
        "                           \"You will then be given several potential responses for the next turn in the conversation. \" \\\n",
        "                           \"These responses all concern an interesting fact, which will be provided as well. Your task is \" \\\n",
        "                           \"to rate each of the responses on several metrics. The response for one metric should not influence \" \\\n",
        "                           \"the other metrics. For example, if a response is not understandable or has grammatical errors \" \\\n",
        "                           \"you should try to ignore this when considering whether it maintains context or if it is interesting. \" \\\n",
        "                           \"Please make sure you read and understand these instructions carefully. Feel free to ask if you require \" \\\n",
        "                           \"clarification. Please keep this document open while reviewing, and refer to it as needed.\"\n",
        "definement_task_description = \"We provided a task where contestants were asked to generate their ideal responses/continuation \" \\\n",
        "                              \"to a conversation, considering the context and an interesting fact provided to them.\" \\\n",
        "                              \" Your task is to define clear and independent metrics that could be used to evaluate \" \\\n",
        "                              \"the quality of these generated continuations.\"\n",
        "scoring_task_description = \"You will be given a conversation between two individuals. \" \\\n",
        "                           \"You will then be given one potential response for the next turn in the conversation. \" \\\n",
        "                           \"The response concerns an interesting fact, which will be provided as well. Your task \" \\\n",
        "                           \"is to rate the responses on one metric.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-aewGrdAI6b"
      },
      "outputs": [],
      "source": [
        "%cd USR\n",
        "import pickle\n",
        "\n",
        "# Save the variables to their corresponding .pkl files\n",
        "with open('content_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(content_column_name, f)\n",
        "\n",
        "with open('default_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(default_task_description, f)\n",
        "\n",
        "with open('definement_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(definement_task_description, f)\n",
        "\n",
        "with open('overall_score_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(overall_score_column_name, f)\n",
        "\n",
        "with open('scoring_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(scoring_task_description, f)\n",
        "\n",
        "with open('response_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(response_column_name, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pTSoWVUAI6c"
      },
      "source": [
        "#### Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7GxNgslAI6c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "usr_data = pd.read_csv('usr.csv')\n",
        "\n",
        "# Select a subset of size 50 with random seed 42\n",
        "test_data = usr_data.sample(n=50, random_state=42)\n",
        "\n",
        "# Save the subset as test.csv\n",
        "test_data.to_csv('test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-25T17:45:00.188427Z",
          "start_time": "2025-09-25T17:45:00.185339Z"
        },
        "id": "-0l83UgpAI6c"
      },
      "source": [
        "class Metric:\n",
        "    def __init__(self, name, description, scale, instruction=\"\"):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.scale = scale\n",
        "        self.instruction = instruction\n",
        "\n",
        "    def set_instruction(self, instruction):\n",
        "        self.instruction = instruction\n",
        "\n",
        "    def to_str(self):\n",
        "        return f\"[Metric]: Name: {self.name}, Description: {self.description}, Scale: {self.scale} [Metric]\\n\"\n",
        "\n",
        "    def to_inst(self):\n",
        "        return f\"[Metric]: Name: {self.name}, Description: {self.description}, Scale: {self.scale} [Metric]\\n{self.instruction}\"\n",
        "\n",
        "\n",
        "def serialize_metric(metric):\n",
        "    return {\n",
        "        \"name\": metric.name,\n",
        "        \"description\": metric.description,\n",
        "        \"scale\": metric.scale,\n",
        "        \"instruction\": metric.instruction\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfDm7M7TAI6d"
      },
      "outputs": [],
      "source": [
        "# Create instances of the Metric class\n",
        "understandable_metric = Metric(\n",
        "    name=\"Understandable\",\n",
        "    description=\"Is the response understandable given the previous context?\",\n",
        "    scale=\"0-1\",\n",
        "    instruction=\"Is the response understandable in the context of the history? (Not if it's on topic, but for example if it uses pronouns they should make sense)\\n\"\n",
        "                \"A score of 0 (no) means that the response is difficult to understand. You do not know what the person is trying to say.\\n\"\n",
        "                \"Example: 'I didn’t know that. I love to watch the movie Inception, it’s also the first racing movie to be a woman haha. I guess the movie was originally titled “Inception” awesome movie!'\\n\"\n",
        "                \"Context: 'In my religion, there is no star. How about you?'\\n\"\n",
        "                \"Response: 'Yeah it was back in 1975.'\\n\"\n",
        "                \"A score of 1 (yes) means that the response is understandable. You know what the person is trying to say.\\n\"\n",
        "                \"Example: 'My favorite role would have to be quarterback. It is such an interesting role.'\\n\"\n",
        "                \"Response: 'That is true. I think LeBron is the highest paid celebrity, I wonder if he will be in the Space Jam sequel.'\"\n",
        ")\n",
        "\n",
        "natural_metric = Metric(\n",
        "    name=\"Natural\",\n",
        "    description=\"Does the response seem to be something that a person would naturally say?\",\n",
        "    scale=\"1-3\",\n",
        "    instruction=\"Is the response naturally written?\\n\"\n",
        "                \"A score of 1 (bad) means that the response is unnatural.\\n\"\n",
        "                \"Context: A: wow. do you believe in stars of the zodiac? what is your star?\\n\"\n",
        "                \"B: in my religion, there is no star. how about you?\\n\"\n",
        "                \"Response: yeah, it was back in 1975.\\n\"\n",
        "                \"i think he is, he is a great teacher and he also taught ellie kemper, she is a great teacher\\n\"\n",
        "                \"A score of 2 (ok) means the response is strange, but not entirely unnatural.\\n\"\n",
        "                \"Context: A: wow. do you believe in stars of the zodiac? what is your star?\\n\"\n",
        "                \"B: in my religion, there is no star. how about you?\\n\"\n",
        "                \"Response: i read it sometimes for the fun of it.\\n\"\n",
        "                \"A score of 3 (good) means that the response is natural.\\n\"\n",
        "                \"i think it’s funny that the soviet union sent a spacecraft to venus\"\n",
        ")\n",
        "maintains_context_metric = Metric(\n",
        "    name=\"Maintains Context\",\n",
        "    description=\"Does the response serve as a valid continuation of the preceding conversation?\",\n",
        "    scale=\"1-3\",\n",
        "    instruction=\"Does the response serve as a valid continuation of the conversation history?\\n\"\n",
        "                \"A score of 1 (no) means that the response drastically changes topic or ignores the conversation history.\\n\"\n",
        "                \"Context: A: wow. do you believe in stars of the zodiac? what is your star?\\n\"\n",
        "                \"B: in my religion, there is no star. how about you?\\n\"\n",
        "                \"Response: i think it’s funny that the soviet union sent a spacecraft to venus.\\n\"\n",
        "                \"A score of 2 (somewhat) means the response refers to the conversation history in a limited capacity (e.g., in a generic way) and shifts the conversation topic.\\n\"\n",
        "                \"Context: i do like some drama stuff, yeah he was awesome in that.\\n\"\n",
        "                \"Response: yeah. do you like jon hamm?\\n\"\n",
        "                \"Context: i believe that! he would have played longer i’m sure if he did the granny style approach to shooting free throws!\\n\"\n",
        "                \"Response: i agree. did you know that space jam is the highest grossing basketball movie of all time?\\n\"\n",
        "                \"A score of 3 (yes) means the response is on topic and strongly acknowledges the conversation history.\\n\"\n",
        "                \"Context: B: wow, that’s great. especially because more than 60% of NBA players go broke 5 years after retirement.\\n\"\n",
        "                \"A: i believe that! he would have played longer i’m sure if he did the granny style approach to shooting free throws!\\n\"\n",
        "                \"Response: a lot of players can make money by starring in movies. did you know space jam is the highest grossing movie of all time? maybe one of the broke retired players can be in the sequel!\\n\"\n",
        "                \"Context: B: you like drama? patrick stewart teaches classes now. i loved him in star trek.\\n\"\n",
        "                \"A: i do like some drama stuff, yeah he was awesome in that.\\n\"\n",
        "                \"Response: jon hamm was also a drama teacher. he taught erin from the office.\"\n",
        ")\n",
        "\n",
        "interesting_metric = Metric(\n",
        "    name=\"Interesting\",\n",
        "    description=\"Is the response dull or interesting?\",\n",
        "    scale=\"1-3\",\n",
        "    instruction=\"Is the response dull/interesting?\\n\"\n",
        "                \"A score of 1 (dull) means that the response is generic and dull.\\n\"\n",
        "                \"Example: 'that's true. i agree.'\\n\"\n",
        "                \"A score of 2 (somewhat interesting) means the response is somewhat interesting and could engage you in the conversation (e.g., an opinion, thought).\\n\"\n",
        "                \"Example: 'my favorite role would have to be quarterback. it is such an interesting role.'\\n\"\n",
        "                \"'i love tom brady. i love tom brady.'\\n\"\n",
        "                \"A score of 3 (interesting) means the response is very interesting or presents an interesting fact.\\n\"\n",
        "                \"Example: 'i agree. did you know that space jam is the highest grossing basketball movie of all time?'\\n\"\n",
        "                \"'a lot of players can make money by starring in movies. did you know space jam is the highest grossing movie of all time? maybe one of the broke retired players can be in the sequel!'\"\n",
        ")\n",
        "\n",
        "uses_knowledge_metric = Metric(\n",
        "    name=\"Uses Knowledge\",\n",
        "    description=\"Given the fact that the response is conditioned on, how well does the response use that fact?\",\n",
        "    scale=\"0-1\",\n",
        "    instruction=\"Given the interesting fact that the response is conditioned on, how well does the response use the fact?\\n\"\n",
        "                \"A score of 0 (no) means the response does not mention or refer to the fact at all.\\n\"\n",
        "                \"A score of 1 (yes) means the response uses the fact well.\"\n",
        ")\n",
        "\n",
        "overall_quality_metric = Metric(\n",
        "    name=\"Overall Quality\",\n",
        "    description=\"Given your answers above, what is your overall impression of the quality of this utterance?\",\n",
        "    scale=\"1-5\",\n",
        "    instruction=\"Given your answers above, what is your overall impression of this utterance?\\n\"\n",
        "                \"A score of 1 (very bad) means this is a completely invalid response. It would be difficult to recover the conversation after this.\\n\"\n",
        "                \"A score of 2 (bad) means this is a valid response, but otherwise poor in quality.\\n\"\n",
        "                \"A score of 3 (neutral) means this response is neither good nor bad. This response has no negative qualities, but no positive ones either.\\n\"\n",
        "                \"A score of 4 (good) means this is a good response, but falls short of being perfect because of a key flaw.\\n\"\n",
        "                \"A score of 5 (very good) means this response is good and does not have any strong flaws.\"\n",
        ")\n",
        "\n",
        "metrics = [understandable_metric, natural_metric, maintains_context_metric, interesting_metric, uses_knowledge_metric,\n",
        "           overall_quality_metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YN5Dxt8AI6e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "results = [serialize_metric(metric) for metric in metrics]\n",
        "json.dump(results, open('ground_truth_metrics_set.json', \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoivzyZ8AI6e"
      },
      "source": [
        "### SumPubMed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_t24jkgAI6e"
      },
      "source": [
        "We designed a task where contestants where asked to generate summaries for biomedical research papers. These documents are sourced from diverse literature, including medline, life science journals, and online books. Moreover, these documents were related to medicine, pharmacy, nursing, dentistry, health care, health services, etc. Now we want to assess these provided summaries. Based on this information define a set of metrics that we should use to assess the provided answers by contestants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRh2_FWAAI6f"
      },
      "outputs": [],
      "source": [
        "#### Definement\n",
        "content_column_name = \"text\"\n",
        "response_column_name = \"shorter_abstract\"\n",
        "overall_score_column_name = \"IOF\"\n",
        "default_task_description = \"We designed a task where contestants where asked to generate \" \\\n",
        "                           \"summaries for biomedical research papers. These documents are sourced from diverse literature, \" \\\n",
        "                           \"including medline, life science journals, and online books. Moreover, these documents were related \" \\\n",
        "                           \"to medicine, pharmacy, nursing, dentistry, health care, health services, etc. Now we want to assess \" \\\n",
        "                           \"these provided summaries. Your task is to rate each of the abstracts on several metrics. The response \" \\\n",
        "                           \"for one metric should not influence the other metrics. For example, if an abstract is not readable enough \" \\\n",
        "                           \"or has grammatical errors you should try to ignore this when considering whether it is informative or \" \\\n",
        "                           \"if it is coherent. Please make sure you read and understand these instructions carefully. Feel free to \" \\\n",
        "                           \"ask if you require clarification. Please keep this document open while reviewing, and refer to it as needed.\"\n",
        "\n",
        "definement_task_description = \"We designed a task where contestants where asked to generate \" \\\n",
        "                              \"summaries for biomedical research papers. These documents are sourced from diverse literature, \" \\\n",
        "                              \"including medline, life science journals, and online books. Moreover, these documents were related \" \\\n",
        "                              \"to medicine, pharmacy, nursing, dentistry, health care, health services, etc. Now we want to assess \" \\\n",
        "                              \"these provided summaries. Your task is to define clear and independent metrics that could be used to evaluate \" \\\n",
        "                              \"the quality of these generated continuations.\"\n",
        "\n",
        "scoring_task_description = \"We designed a task where contestants where asked to generate \" \\\n",
        "                           \"summaries for biomedical research papers. These documents are sourced from diverse literature, \" \\\n",
        "                           \"including medline, life science journals, and online books. Moreover, these documents were related \" \\\n",
        "                           \"to medicine, pharmacy, nursing, dentistry, health care, health services, etc. Now we want to assess \" \\\n",
        "                           \"these provided summaries. Your task is to rate the responses on one metric.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h8F8WpbAI6f"
      },
      "outputs": [],
      "source": [
        "%cd SumPubMed\n",
        "import pickle\n",
        "\n",
        "# Save the variables to their corresponding .pkl files\n",
        "with open('content_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(content_column_name, f)\n",
        "\n",
        "with open('default_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(default_task_description, f)\n",
        "\n",
        "with open('definement_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(definement_task_description, f)\n",
        "\n",
        "with open('overall_score_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(overall_score_column_name, f)\n",
        "\n",
        "with open('scoring_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(scoring_task_description, f)\n",
        "\n",
        "with open('response_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(response_column_name, f)\n",
        "#### Scoring\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "spm_data = pd.read_csv('sumpubmed.csv')\n",
        "\n",
        "# Save the subset as test.csv\n",
        "spm_data.to_csv('test.csv', index=False)\n",
        "\n",
        "non_re_metric = Metric(\n",
        "    name=\"Non-Repetition and no factual Redundancy\",\n",
        "    description=\"There should not be redundancy in the factual information, and no repetition of sentences is allowed.\",\n",
        "    scale=\"1-10\"\n",
        ")\n",
        "\n",
        "coherence_metric = Metric(\n",
        "    name=\"Coherence\",\n",
        "    description=\"Coherence means 'continuity of sense'. The arguments have to be connected sensibly so that the reader can see consecutive sentences as being about one (or a related) concept.\",\n",
        "    scale=\"1-10\"\n",
        ")\n",
        "\n",
        "readability_metric = Metric(\n",
        "    name=\"Readability\",\n",
        "    description=\"Consideration of general readability criteria such as good spelling, correct grammar, understandability, etc. in the summaries.\",\n",
        "    scale=\"1-10\"\n",
        ")\n",
        "\n",
        "informativeness_metric = Metric(\n",
        "    name=\"Informativeness, Overlap and Focus\",\n",
        "    description=\"How much information is covered by the summary. The goal is to find the common pieces of information via matching the same keywords (or key phrases), such as 'Nematodes', across the summary. For overlaps, annotators compare the keywords’ (or key-phrases) occurrence frequency and ensure the summaries are on the same topic.\",\n",
        "    scale=\"1-10\"\n",
        ")\n",
        "\n",
        "overall_quality_metric = Metric(\n",
        "    name=\"Overall Quality\",\n",
        "    description=\"Given your answers above, what is your overall impression of the quality of this summary?\",\n",
        "    scale=\"1-10\"\n",
        ")\n",
        "\n",
        "metrics = [non_re_metric, coherence_metric, readability_metric, informativeness_metric, overall_quality_metric]\n",
        "import json\n",
        "import os\n",
        "\n",
        "results = [serialize_metric(metric) for metric in metrics]\n",
        "json.dump(results, open('ground_truth_metrics_set.json', \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1D0ESQ1AI6f"
      },
      "source": [
        "### SummEval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-25T17:55:55.090442Z",
          "start_time": "2025-09-25T17:55:55.045930Z"
        },
        "id": "_ZNhJyAfAI6g"
      },
      "source": [
        "#### Definement\n",
        "content_column_name = \"article\"\n",
        "response_column_name = \"decoded\"\n",
        "overall_score_column_name = \"expert_relevance\"\n",
        "default_task_description = \"In this task you will evaluate the quality of summaries written \" \\\n",
        "                           \"for a news article. To correctly solve the task, follow these steps:\\n\" \\\n",
        "                           \"1. Carefully read the news article, be aware of the information it contains.\\n\" \\\n",
        "                           \"2. Read the proposed summary.\\n\" \\\n",
        "                           \"3. Rate the summary on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\"\n",
        "definement_task_description = \"We provided a task where contestants were asked to generate their ideal summary \" \\\n",
        "                              \"for a news article.\" \\\n",
        "                              \" Your task is to define clear and independent metrics that could be used to evaluate \" \\\n",
        "                              \"the quality of these generated summaries.\"\n",
        "scoring_task_description = \"We provided a task where contestants were asked to generate their ideal summary \" \\\n",
        "                           \"for a news article. Your task is to rate the responses on one metric.\"\n",
        "\n",
        "%cd SummEval\n",
        "import pickle\n",
        "\n",
        "# Save the variables to their corresponding .pkl files\n",
        "with open('content_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(content_column_name, f)\n",
        "\n",
        "with open('default_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(default_task_description, f)\n",
        "\n",
        "with open('definement_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(definement_task_description, f)\n",
        "\n",
        "with open('overall_score_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(overall_score_column_name, f)\n",
        "\n",
        "with open('scoring_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(scoring_task_description, f)\n",
        "\n",
        "with open('response_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(response_column_name, f)\n",
        "\n",
        "#### Scoring\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "usr_data = pd.read_csv('summeval.csv')\n",
        "\n",
        "# Select a subset of size 50 with random seed 42\n",
        "#test_data = usr_data.sample(n=50, random_state=42)\n",
        "\n",
        "# Save the subset as test.csv\n",
        "#test_data.to_csv('test.csv', index=False)\n",
        "\n",
        "# Create instances of the Metric class\n",
        "coherence_metric = Metric(\n",
        "    name=\"Coherence\",\n",
        "    description=\"The rating measures the quality of all sentences collectively, to the fit together and sound naturally. Consider the quality of the summary as a whole.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "consistency_metric = Metric(\n",
        "    name=\"Consistency\",\n",
        "    description=\"The rating measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "fluency_metric = Metric(\n",
        "    name=\"Fluency\",\n",
        "    description=\"This rating measure the quality of individual sentences. Are they well-written and grammatically correct? Consider the quality of individual sentences.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "relevance_metric = Metric(\n",
        "    name=\"Relevance\",\n",
        "    description=\"The rating measures how well the summary captures the key points of the article. Consider whether all and only the important aspects are contained in the summary.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "overall_quality_metric = Metric(\n",
        "    name=\"Overall Quality\",\n",
        "    description=\"Given your answers above, what is your overall impression of the quality of this utterance?\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "ground_truth_metrics_set = [relevance_metric, consistency_metric, fluency_metric, coherence_metric,\n",
        "                            overall_quality_metric]\n",
        "\n",
        "import json\n",
        "\n",
        "results = [serialize_metric(metric) for metric in ground_truth_metrics_set]\n",
        "json.dump(results, open('ground_truth_metrics_set.json', \"w\"), indent=4)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-25T17:56:08.386666Z",
          "start_time": "2025-09-25T17:56:08.382064Z"
        },
        "id": "HzLEntZNAI6g"
      },
      "cell_type": "code",
      "source": [
        "coherence_metric = Metric(\n",
        "    name=\"Coherence\",\n",
        "    description=\"The rating measures the quality of all sentences collectively, to the fit together and sound naturally. Consider the quality of the summary as a whole.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "consistency_metric = Metric(\n",
        "    name=\"Consistency\",\n",
        "    description=\"The rating measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "fluency_metric = Metric(\n",
        "    name=\"Fluency\",\n",
        "    description=\"This rating measure the quality of individual sentences. Are they well-written and grammatically correct? Consider the quality of individual sentences.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "relevance_metric = Metric(\n",
        "    name=\"Relevance\",\n",
        "    description=\"The rating measures how well the summary captures the key points of the article. Consider whether all and only the important aspects are contained in the summary.\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "overall_quality_metric = Metric(\n",
        "    name=\"Overall Quality\",\n",
        "    description=\"Given your answers above, what is your overall impression of the quality of this utterance?\",\n",
        "    scale=\"1-5\"\n",
        ")\n",
        "\n",
        "ground_truth_metrics_set = [relevance_metric, consistency_metric, fluency_metric, coherence_metric,\n",
        "                            overall_quality_metric]\n",
        "\n",
        "import json\n",
        "\n",
        "results = [serialize_metric(metric) for metric in ground_truth_metrics_set]\n",
        "json.dump(results, open('ground_truth_metrics_set.json', \"w\"), indent=4)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwpa7i6OAI6g"
      },
      "source": [
        "### HelpSteer2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDa0kP1GAI6g"
      },
      "outputs": [],
      "source": [
        "#### Definement\n",
        "content_column_name = \"prompt\"\n",
        "response_column_name = \"response\"\n",
        "overall_score_column_name = \"helpfulness\"\n",
        "default_task_description = \"You will be given prompts/instructions and a response from AI systems/humans. Your task consists of: \"\n",
        "definement_task_description = \"We provided a task where contestants were asked to respond to prompts/instructions provided by users. \" \\\n",
        "                              \"Your task is to define clear and independent metrics that could be used to evaluate \" \\\n",
        "                              \"the quality of these generated continuations.\"\n",
        "scoring_task_description = \"We provided a task where contestants were asked to respond to prompts/instructions provided by users. \" \\\n",
        "                           \"Your task is to rate the responses on one metric.\"\n",
        "%cd HelpSteer2\n",
        "import pickle\n",
        "\n",
        "# Save the variables to their corresponding .pkl files\n",
        "with open('content_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(content_column_name, f)\n",
        "\n",
        "with open('default_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(default_task_description, f)\n",
        "\n",
        "with open('definement_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(definement_task_description, f)\n",
        "\n",
        "with open('overall_score_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(overall_score_column_name, f)\n",
        "\n",
        "with open('scoring_task_description.pkl', 'wb') as f:\n",
        "    pickle.dump(scoring_task_description, f)\n",
        "\n",
        "with open('response_column_name.pkl', 'wb') as f:\n",
        "    pickle.dump(response_column_name, f)\n",
        "\n",
        "#### Scoring\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "usr_data = pd.read_csv('helpsteer2.csv')\n",
        "\n",
        "# Select a subset of size 50 with random seed 42\n",
        "test_data = usr_data.sample(n=50, random_state=42)\n",
        "\n",
        "# Save the subset as test.csv\n",
        "test_data.to_csv('test.csv', index=False)\n",
        "\n",
        "helpfulness_metric = Metric(\n",
        "    name=\"Helpfulness/Understanding\",\n",
        "    description=\"How useful and helpful the response is (“overall quality rating”)\",\n",
        "    scale=\"0-4\",\n",
        "    instruction=\"\"\"\n",
        "    1. Helpfulness/Understanding\n",
        "    • 4–The response is extremely helpful and completely aligned with the spirit of what the prompt was asking for.\n",
        "    • 3–The response is mostly helpful and mainly aligned with what the user was looking for, but there is still some room for improvement.\n",
        "    • 2–The response is partially helpful but misses the overall goal of the user’s query/input in some way. The response did not fully satisfy what the user was looking for.\n",
        "    • 1–The response is borderline unhelpful and mostly does not capture what the user was looking for, but it is still usable and helpful in a small way.\n",
        "    • 0–The response is not useful or helpful at all. The response completely missed the essence of what the user wanted.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "correctness_metric = Metric(\n",
        "    name=\"Correctness/Completeness\",\n",
        "    description=\"The response is based on facts, no hallucinations, no mistakes. The response covers everything required in the instruction.\",\n",
        "    scale=\"0-4\",\n",
        "    instruction=\"\"\"\n",
        "    2. Correctness/Completeness\n",
        "    • 4– The response is completely correct and accurate to what is requested by the prompt with no necessary details missing and without false, misleading, or hallucinated information. If the prompt asks the assistant to do a task, the task is completely done and addressed in the response.\n",
        "    • 3–The response is mostly accurate and correct with a small amount of missing information. It contains no misleading information or hallucinations. If the prompt asks the assistant to perform a task, the task is mostly successfully attempted.\n",
        "    • 2– The response contains a mix of correct and incorrect information. The response may miss some details, contain misleading information, or minor hallucinations, but is more or less aligned with what the prompt asks for. If the prompt asks the assistant to perform a task, the task is attempted with moderate success but still has clear room for improvement.\n",
        "    • 1– The response has some correct elements but is mostly wrong or incomplete. The response may contain multiple instances of hallucinations, false information, misleading information, or irrelevant information. If the prompt asks the assistant to do a task, the task was attempted with a small amount of success.\n",
        "    • 0–The response is completely incorrect. All information provided is wrong, false or hallucinated. If the prompt asks the assistant to do a task, the task is not at all attempted, or the wrong task was attempted in the response. The response is completely irrelevant to the prompt.\n",
        "    • We also have a rating confidence check box where you can provide how confident you are in your correctness assessment:\n",
        "    (a) Very confident\n",
        "    (b) Somewhat confident\n",
        "    (c) Not confident/Unsure (use it when unable to verify the correctness of key information provided in the response)\n",
        "    • Additionally, we have binary check boxes that should be checked if they apply to the given response. The check boxes include:\n",
        "    (a) Contains incorrect information\n",
        "    (b) Contains irrelevant information\n",
        "    (c) Key information is missing\n",
        "    (d) Instruction is based on a false premise\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "coherence_clarity_metric = Metric(\n",
        "    name=\"Coherence/Clarity\",\n",
        "    description=\"The response is self consistent in terms of content, style of writing, and does not contradict itself. The response can be logically followed and understood by a human. The response does not contain redundant or repeated information.\",\n",
        "    scale=\"0-4\",\n",
        "    instruction=\"\"\"\n",
        "    3. Coherence/Clarity\n",
        "    With this attribute we measure how lucid, cogent, and self-consistent the model’s response is. This attribute will be particularly varied for open-ended questions, tasks, and objectives like writing a story, generating a dialogue, or summary but also applies to more straightforward prompt/response pairs.\n",
        "    • 4 (Perfectly Coherent and Clear)– The response is perfectly clear and self-consistent throughout. There are no contradictory assertions or statements, the writing flows logically and following the train of thought/story is not challenging.\n",
        "    • 3 (Mostly Coherent and Clear)– The response is mostly clear and coherent, but there may be one or two places where the wording is confusing or the flow of the response is a little hard to follow. Over all, the response can mostly be followed with a little room for improvement.\n",
        "    • 2 (A Little Unclear and/or Incoherent)– The response is a little unclear. There are some inconsistencies or contradictions, run on sentences, confusing statements, or hard to follow sections of the response.\n",
        "    • 1 (Mostly Incoherent and/or Unclear)– The response is mostly hard to follow, with inconsistencies, contradictions, confusing logic flow, or unclear language used throughout, but there are some coherent/clear parts.\n",
        "    • 0 (Completely Incoherent and/or Unclear)– The response is completely incomprehensible and no clear meaning or sensible message can be discerned from it.\n",
        "    • Additionally has binary checkboxes for:\n",
        "    (a) Contains repetitions\n",
        "    (b) Contains style changes\n",
        "    (c) Contains contradiction(s)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "simple_complex_language_metric = Metric(\n",
        "    name=\"Simple vs. Complex Language\",\n",
        "    description=\"Rate the response along a simple → complex spectrum. The response uses simple, easy to understand vocabulary and sentence structure that children can understand vs the model uses sophisticated language with elevated vocabulary that adults with advanced education or experts on the topic would use.\",\n",
        "    scale=\"0-4\",\n",
        "    instruction=\"\"\"\n",
        "    4. Simple/Complex Language\n",
        "    • 4 (Expert)– An expert in the field or area could have written the response. It uses specific and technically relevant vocabulary. Elevated language that someone at the simple or basic level may not understand at all. The professional language of a lawyer, scientist, engineer, or doctor falls into this category.\n",
        "    • 3 (Advanced)– The response uses a fairly sophisticated vocabulary and terminology. Someone majoring in this subject at a college or university could have written it and would understand the response. An average adult who does not work or study in this area could not have written the response.\n",
        "    • 2 (Intermediate)– People who have completed up through a high school education will probably be able to understand the vocabulary and sentence structure used, but those at the basic level or children might struggle to understand the response.\n",
        "    • 1 (Simple)– The response uses relatively straightforward language and wording, but some schooling through elementary or a middle school in the language might be required to understand the response.\n",
        "    • 0 (Basic)– The response uses very easy to understand language that is clear and completely interpretable by children, adults, and anyone with a functional command of the language.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "succinct_verbose_language_metric = Metric(\n",
        "    name=\"Succinct vs. Verbose Language\",\n",
        "    description=\"The response is direct to the point without extra wordings. The opposite direction is verbose, the response is wordy, giving a long winded and/or detailed reply.\",\n",
        "    scale=\"0-4\",\n",
        "    instruction=\"\"\"\n",
        "    5. Succinctness/Verbosity\n",
        "    The goal here is to place the response on a spectrum from the most short, crisp answers, to the most lengthy, detailed, and/or wordy answers under the context of what a user is expecting as a response to the prompt. For example, if the prompt asks the model a yes or no question and the model simply responds “yes” the answer is succinct. But if the model responds “yes”, restates the question worded as an answer, and explains why it gave that answer, the answer is verbose. Even if two responses have exactly the same length, one can be rated as verbose and the other as succinct depending on the prompting context. This verbosity rating evaluates the response as a whole (ex: a very long list of items would usually be considered verbose even if each item in the list is described succinctly).\n",
        "    • 4 (Verbose)– The response is particularly lengthy, wordy, and/or extensive with extra details given what the prompt requested from the assistant model. The response can be verbose regardless of if the length is due to repetition and incoherency or if it is due to rich and insightful detail.\n",
        "    • 3 (Moderately Long)– The response is on the longer side but could still have more added to it before it is considered fully detailed or rambling.\n",
        "    • 2 (Average Length)–The response isn’t especially long or short given what the prompt is asking of the model. The length is adequate for conveying a full response but isn’t particularly wordy nor particularly concise.\n",
        "    • 1 (Pretty Short)– The response is on the shorter side but could still have words, details, and/or text removed before it’s at a bare minimum of what the response is trying to convey.\n",
        "    • 0 (Succinct)– The response is short, to the point, and the most concise it can be. No additional information is provided outside of what is requested by the prompt (regardless of if the information or response itself is incorrect, hallucinated, or misleading. A response that gives an incorrect answer can still be succinct.).\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "safe_unsafe_metric = Metric(\n",
        "    name=\"Safe vs Unsafe\",\n",
        "    description=\"Whether the response is safe (“Pass”), i.e, excludes any kind of harmful / toxic / illegal content, or is not (“Fail”).\",\n",
        "    scale=\"pass/fail\",\n",
        "    instruction=\"\"\"\n",
        "    6. Safe/Unsafe\n",
        "    • Pass (Safe)– The response does not include any of the unsafe content described below\n",
        "    • Fail (Unsafe)–  Harmful Content (response includes hate speech, bullying, harassment, sexually explicit content, or violence), illegal activities (response promotes any illegal activities or harmful actions to individuals’ or groups’ mental, physical, or emotional health), profanity (response contains any curse words), bias and stereotyping (response promotes and/or perpetuate biases and/or stereotypical responses on select societal groups)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "ground_truth_metrics_set = [correctness_metric, coherence_clarity_metric, simple_complex_language_metric,\n",
        "                            succinct_verbose_language_metric, safe_unsafe_metric, helpfulness_metric]\n",
        "\n",
        "import json\n",
        "\n",
        "results = [serialize_metric(metric) for metric in ground_truth_metrics_set]\n",
        "json.dump(results, open('ground_truth_metrics_set.json', \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8BGhHgfAI6g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-evaluation-KmishpgR-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}