# llm-generated-rubrics
This repository contains reusable implementations of the evaluation metrics introduced in Learning to Judge: LLMs Designing and Applying Evaluation Rubrics. The metrics support the analysis of LLM-defined evaluation criteria, their alignment with human rubrics, and the reliability of LLM-based scoring.
