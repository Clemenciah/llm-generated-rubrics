# GER-Eval Metrics

This repository provides reference implementations of the evaluation metrics introduced in  **Learning to Judge: LLMs Designing and Applying Evaluation Rubrics**.

Large language models (LLMs) are increasingly used as evaluators for natural language generation, typically applying human-defined rubrics to assess system outputs. GER-Eval investigates whether LLMs can instead *design* and *apply* their own evaluation rubrics, and how such LLM-defined criteria compare to human-defined ones in terms of reliability and alignment.
